{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module: main_TSC_LS.ipynb\n",
    "Author: Dema N. Govalla\n",
    "Date: December 4, 2023\n",
    "Description: The file trains and test the combined_sensorData.csv file using the TSC-LS algorithm. \n",
    "            After traning and testing, it returns the algorithms metrics such as accuracy, presision and more.\n",
    "            The file performs cross validation for different TSC-LS parameters and returns the classification\n",
    "            report. \n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Start of Cross validation results\n",
    "'''\n",
    "import numpy as np, pandas as pd, random, sys, seaborn as sns, os, statistics, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from TSC_LS import LearningShapelets\n",
    "from time import time\n",
    "import warnings, matplotlib.cm as cm\n",
    "import matplotlib, shutil\n",
    "warnings.filterwarnings(\"ignore\", category=matplotlib.MatplotlibDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "# np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report_def = classification_report(y_test, y_pred)\n",
    "    return mae, mse, rmse, r2, accuracy, precision, recall, f1, conf_matrix, class_report_def\n",
    "\n",
    "def plot_accuracy_bar_graph(values_array, dist_measure_strings, plot_title, save_filename):\n",
    "    directory_path_result = 'TSC_LS_result'\n",
    "    if not os.path.exists(directory_path_result):\n",
    "        os.makedirs(directory_path_result)\n",
    "        \n",
    "    num_gammas = len(values_array[0])\n",
    "    num_thetas = len(values_array)\n",
    "\n",
    "    bar_width = 0.06\n",
    "    index = np.arange(num_gammas)\n",
    "    colors = plt.cm.get_cmap('tab10', num_thetas)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(num_thetas):\n",
    "        bars = plt.bar(index + i * bar_width, values_array[i], width=bar_width, label=f'Dist_measure = {dist_measure_strings[i]}', color=colors(i))\n",
    "        for bar, acc in zip(bars, values_array[i]):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{acc:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "    plt.xlabel('shapelets length')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(plot_title)\n",
    "    plt.xticks(index + (bar_width * (num_thetas - 1)) / 2, [f'shap_len {i+1}' for i in range(num_gammas)])  \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  \n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(directory_path_result+ '/' + save_filename)  \n",
    "\n",
    "def normalize_standard(X, scaler=None):\n",
    "    shape = X.shape\n",
    "    data_flat = X.flatten()\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        data_transformed = scaler.fit_transform(data_flat.reshape(np.prod(shape), 1)).reshape(shape)\n",
    "    else:\n",
    "        data_transformed = scaler.transform(data_flat.reshape(np.prod(shape), 1)).reshape(shape)\n",
    "    return data_transformed, scaler\n",
    "\n",
    "def normalize_data(X, scaler=None):\n",
    "    if scaler is None:\n",
    "        X, scaler = normalize_standard(X)\n",
    "    else:\n",
    "        X, scaler = normalize_standard(X, scaler)\n",
    "    return X, scaler\n",
    "\n",
    "def sample_ts_segments(X, shapelets_size, n_segments=10000):\n",
    "    \"\"\"\n",
    "    Sample time series segments for k-Means.\n",
    "    \"\"\"\n",
    "    n_ts, n_channels, len_ts = X.shape\n",
    "    samples_i = random.choices(range(n_ts), k=n_segments)\n",
    "    segments = np.empty((n_segments, n_channels, shapelets_size))\n",
    "    for i, k in enumerate(samples_i):\n",
    "        s = random.randint(0, len_ts - shapelets_size)\n",
    "        segments[i] = X[k, :, s:s+shapelets_size]\n",
    "    return segments\n",
    "\n",
    "def get_weights_via_kmeans(X, shapelets_size, num_shapelets, n_segments=10000):\n",
    "    \"\"\"\n",
    "    Get weights via k-Means for a block of shapelets.\n",
    "    \"\"\"\n",
    "    segments = sample_ts_segments(X, shapelets_size, n_segments).transpose(0, 2, 1)\n",
    "    k_means = TimeSeriesKMeans(n_clusters=num_shapelets, metric=\"euclidean\", max_iter=50).fit(segments)\n",
    "    clusters = k_means.cluster_centers_.transpose(0, 2, 1)\n",
    "    return clusters\n",
    "\n",
    "def eval_accuracy(model, X, Y):\n",
    "    result = []\n",
    "    predictions = model.predict(X)\n",
    "    if len(predictions.shape) == 2:\n",
    "        predictions = predictions.argmax(axis=1)\n",
    "        print(predictions)\n",
    "    print(type(predictions))\n",
    "    print(predictions.shape)\n",
    "\n",
    "    result.append(predictions)\n",
    "    print(f\"Accuracy: {(predictions == Y).sum() / Y.size}\")\n",
    "    return result\n",
    "\n",
    "def average_of_n_values(arr):\n",
    "    if not arr:\n",
    "        return \"Input array is empty. Please provide values.\"\n",
    "    return (sum(arr) / len(arr))*100\n",
    "\n",
    "def convert_seconds_to_minutes(seconds):\n",
    "    minutes = seconds // 60\n",
    "    remaining_seconds = seconds % 60\n",
    "    result = f\"{minutes} minute{'s' if minutes != 1 else ''}\"\n",
    "    if remaining_seconds > 0:\n",
    "        result += f\" and {remaining_seconds} second{'s' if remaining_seconds != 1 else ''}\"\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of CPUs in the system: {}'.format(os.cpu_count()))\n",
    "\n",
    "sensor_data = pd.read_csv('combined_sensorData.csv')\n",
    "sensor_data = sensor_data.iloc[:,0:]\n",
    "\n",
    "# separate the independent and dependent features\n",
    "X_def = sensor_data.iloc[:, np.r_[0:4, 8]]\n",
    "y_def = sensor_data.iloc[:, sensor_data.shape[1]-1]\n",
    "\n",
    "# separate the independent and dependent features\n",
    "X_tex = sensor_data.iloc[:, np.r_[2:9]] \n",
    "y_tex = sensor_data.iloc[:, sensor_data.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path_param = 'cross_val_TSC_LS_param'\n",
    "if os.path.exists(directory_path_param):\n",
    "    shutil.rmtree(directory_path_param)\n",
    "    os.makedirs(directory_path_param)\n",
    "else:\n",
    "    os.makedirs(directory_path_param)\n",
    "\n",
    "n_splits=2\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle= True, random_state=42)\n",
    "\n",
    "fold_no_def = 1\n",
    "for train_index_def, val_index_def in skf.split(X_def, y_def):\n",
    "    train_def = X_def.loc[train_index_def,:]\n",
    "    val_def = X_def.loc[val_index_def,:]\n",
    "    train_def.to_csv(os.path.join(directory_path_param, 'train_fold_def_' + str(fold_no_def) + '.csv'))\n",
    "    val_def.to_csv(os.path.join(directory_path_param, 'val_fold_def_' + str(fold_no_def) + '.csv'))\n",
    "    fold_no_def += 1\n",
    "\n",
    "fold_no_tex = 1\n",
    "for train_index_tex, val_index_tex in skf.split(X_tex, y_tex):\n",
    "    train_tex = X_tex.loc[train_index_tex,:]\n",
    "    val_tex = X_tex.loc[val_index_tex,:]\n",
    "    train_tex.to_csv(os.path.join(directory_path_param, 'train_fold_tex_' + str(fold_no_tex) + '.csv'))\n",
    "    val_tex.to_csv(os.path.join(directory_path_param, 'val_fold_tex_' + str(fold_no_tex) + '.csv'))\n",
    "    fold_no_tex += 1   \n",
    "\n",
    "count = 0\n",
    "for path in os.listdir(directory_path_param):\n",
    "    if os.path.isfile(os.path.join(directory_path_param, path)):\n",
    "        count += 1\n",
    "print('File count:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_measure_strings = [\"euclidean\", \"cosine\"]  \n",
    "shapelets_len_values = [2, 3]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "k-fold parameters for deformation\n",
    "'''\n",
    "k_fold_mae_def = []\n",
    "k_fold_mse_def = []\n",
    "k_fold_rmse_def = []\n",
    "k_fold_r2_def = []\n",
    "\n",
    "k_fold_accuracy_def = []\n",
    "k_fold_precision_def = []\n",
    "k_fold_recall_def = []\n",
    "k_fold_f1_def = []\n",
    "\n",
    "'''\n",
    "k-fold parameters for texture\n",
    "'''\n",
    "k_fold_mae_tex = []\n",
    "k_fold_mse_tex = []\n",
    "k_fold_rmse_tex = []\n",
    "k_fold_r2_tex = []\n",
    "\n",
    "k_fold_accuracy_tex = []\n",
    "k_fold_precision_tex = []\n",
    "k_fold_recall_tex = []\n",
    "k_fold_f1_tex = []\n",
    "\n",
    "'''\n",
    "Metric value arrays for deformation\n",
    "'''\n",
    "mae_values_array_def = []\n",
    "mse_values_array_def = []\n",
    "rmse_values_array_def = []\n",
    "r2_values_array_def = []\n",
    "\n",
    "accuracy_values_array_def = []\n",
    "precision_values_array_def = []\n",
    "recall_values_array_def = []\n",
    "f1_values_array_def = []\n",
    "\n",
    "'''\n",
    "Metric value arrays for texture\n",
    "'''\n",
    "mae_values_array_tex = []\n",
    "mse_values_array_tex = []\n",
    "rmse_values_array_tex = []\n",
    "r2_values_array_tex = []\n",
    "\n",
    "accuracy_values_array_tex = []\n",
    "precision_values_array_tex = []\n",
    "recall_values_array_tex = []\n",
    "f1_values_array_tex = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Full Deformation code for TSC_LS\n",
    "'''\n",
    "for dist_measure in dist_measure_strings:\n",
    "    startTime4 = time() \n",
    "    \n",
    "    '''\n",
    "    Metric value for deformation\n",
    "    '''\n",
    "    mae_values_def = []\n",
    "    mse_values_def = []\n",
    "    rmse_values_def = []\n",
    "    r2_values_def = []\n",
    "    \n",
    "    accuracy_values_def = []\n",
    "    precision_values_def = []\n",
    "    recall_values_def = []\n",
    "    f1_values_def = []\n",
    "    \n",
    "    for shapelets_len in shapelets_len_values:\n",
    "        \n",
    "        startTime3 = time() # Add a timer. \n",
    "        \n",
    "        '''\n",
    "        Metric ave for deformation\n",
    "        '''\n",
    "        mae_ave_def = []\n",
    "        mse_ave_def = []\n",
    "        rmse_ave_def = []\n",
    "        r2_ave_def = []\n",
    "        \n",
    "        accuracy_ave_def = []\n",
    "        precision_ave_def = []\n",
    "        recall_ave_def = []\n",
    "        f1_ave_def = []\n",
    "    \n",
    "        for fold_no in range(1,n_splits+1):\n",
    "            \n",
    "            startTime2 = time() # Add a timer.\n",
    "\n",
    "            '''\n",
    "            Train the deformation data\n",
    "            '''\n",
    "            newtrain_def = pd.read_csv(os.path.join(directory_path_param, 'train_fold_def_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newtrain_def = newtrain_def.columns[1:-1]\n",
    "            combined_array_newtrain_def = []\n",
    "            \n",
    "            for column in columns_to_combine_newtrain_def:\n",
    "                new_array_def = newtrain_def[column].to_numpy()\n",
    "                combined_array_newtrain_def.append(new_array_def)\n",
    "\n",
    "            combined_array_newtrain_def = np.column_stack(combined_array_newtrain_def)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_train_def = np.array([label_map[label] for label in newtrain_def.iloc[:,-1].to_numpy()])\n",
    "            new_X_train_def = combined_array_newtrain_def.reshape(new_y_train_def.size, columns_to_combine_newtrain_def.size,1)\n",
    "            new_X_train_def, scaler_def = normalize_data(new_X_train_def)\n",
    "            \n",
    "        \n",
    "            n_ts_def, n_channels_def, len_ts_def = new_X_train_def.shape\n",
    "            loss_func_def = nn.CrossEntropyLoss()\n",
    "            num_classes_def = len(set(new_y_train_def))\n",
    "            # learn 2 shapelets of length 130\n",
    "            shapelets_size_and_len_def = {1: shapelets_len}\n",
    "            # dist_measure_def = \"euclidean\"\n",
    "            dist_measure_def = dist_measure\n",
    "            \n",
    "            lr_def = 1e-2\n",
    "            wd_def = 1e-3\n",
    "            epsilon_def = 1e-7\n",
    "        \n",
    "            learning_shapelets_def = LearningShapelets(shapelets_size_and_len=shapelets_size_and_len_def,\n",
    "                                                in_channels=n_channels_def,\n",
    "                                                num_classes=num_classes_def,\n",
    "                                                loss_func=loss_func_def,\n",
    "                                                to_cuda=False,\n",
    "                                                verbose=1,\n",
    "                                                dist_measure=dist_measure_def)\n",
    "\n",
    "            for i, (shapelets_size, num_shapelets) in enumerate(shapelets_size_and_len_def.items()):\n",
    "                weights_block = get_weights_via_kmeans(new_X_train_def, shapelets_size, num_shapelets)\n",
    "                learning_shapelets_def.set_shapelet_weights_of_block(i, weights_block)\n",
    "        \n",
    "            optimizer = optim.Adam(learning_shapelets_def.model.parameters(), lr=lr_def, weight_decay=wd_def, eps=epsilon_def)\n",
    "            learning_shapelets_def.set_optimizer(optimizer)\n",
    "            losses = learning_shapelets_def.fit(new_X_train_def, new_y_train_def, epochs=2000, batch_size=256, shuffle=False, drop_last=False)\n",
    "            print(\"Training is Done\")\n",
    "            \n",
    "            '''\n",
    "            Test the deformation data\n",
    "            '''\n",
    "            newval_def = pd.read_csv(os.path.join(directory_path_param, 'val_fold_def_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newval_def = newval_def.columns[1:-1]\n",
    "            combined_array_newval_def = []\n",
    "            for column in columns_to_combine_newval_def:\n",
    "                new_array_def = newval_def[column].to_numpy()\n",
    "                combined_array_newval_def.append(new_array_def)\n",
    "            combined_array_newval_def = np.column_stack(combined_array_newval_def)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_val_def = np.array([label_map[label] for label in newval_def.iloc[:,-1].to_numpy()])\n",
    "            new_X_val_def = combined_array_newval_def.reshape(new_y_val_def.size,columns_to_combine_newval_def.size,1)\n",
    "            new_X_val_def, scaler = normalize_data(new_X_val_def)\n",
    "\n",
    "            y_predlr_val_def = np.array(eval_accuracy(learning_shapelets_def,new_X_val_def,new_y_val_def)).reshape(-1)\n",
    "\n",
    "            '''\n",
    "            Metrics For Deformation\n",
    "            '''   \n",
    "            mae_def, mse_def, rmse_def, r2_def, accuracy_def, precision_def, recall_def, f1_def, conf_matrix_def, class_report_def = calculate_metrics(new_y_val_def, y_predlr_val_def)\n",
    "            print(\"mae:\", mae_def*100)\n",
    "            print(\"mse:\", mse_def*100)\n",
    "            print(\"rmse:\", rmse_def*100)\n",
    "            print(\"r2:\", r2_def*100)\n",
    "            \n",
    "            print(\"Accuracy:\", accuracy_def*100)\n",
    "            print(\"Precision:\", precision_def*100)\n",
    "            print(\"Recall:\", recall_def*100)\n",
    "            print(\"F1 Score:\", f1_def*100)\n",
    "            print(\"Confusion Matrix:\\n\", conf_matrix_def)\n",
    "            print(\"Classification report:\\n\", class_report_def)\n",
    "            \n",
    "            mae_ave_def.append(mae_def)\n",
    "            mse_ave_def.append(mse_def)\n",
    "            rmse_ave_def.append(rmse_def)\n",
    "            r2_ave_def.append(r2_def)\n",
    "            \n",
    "            accuracy_ave_def.append(accuracy_def)\n",
    "            precision_ave_def.append(precision_def)\n",
    "            recall_ave_def.append(recall_def)\n",
    "            f1_ave_def.append(f1_def)    \n",
    "            \n",
    "            # end_time1 = time()\n",
    "            # totalTime1 = convert_seconds_to_minutes(end_time1 - startTime1)\n",
    "            # print(\"Total time 1 is: \", totalTime1)          \n",
    "\n",
    "        '''\n",
    "        Combine K-fold for deformation\n",
    "        '''\n",
    "        k_fold_mae_def.append(mae_ave_def)\n",
    "        k_fold_mse_def.append(mse_ave_def)\n",
    "        k_fold_rmse_def.append(rmse_ave_def)\n",
    "        k_fold_r2_def.append(r2_ave_def)\n",
    "        \n",
    "        k_fold_accuracy_def.append(accuracy_ave_def)\n",
    "        k_fold_precision_def.append(precision_ave_def)\n",
    "        k_fold_recall_def.append(recall_ave_def)\n",
    "        k_fold_f1_def.append(f1_ave_def)\n",
    "        \n",
    "        '''\n",
    "        Combine values for deformation\n",
    "        '''\n",
    "        mae_values_def.append(average_of_n_values(mae_ave_def)) \n",
    "        mse_values_def.append(average_of_n_values(mse_ave_def)) \n",
    "        rmse_values_def.append(average_of_n_values(rmse_ave_def)) \n",
    "        r2_values_def.append(average_of_n_values(r2_ave_def)) \n",
    "        \n",
    "        accuracy_values_def.append(average_of_n_values(accuracy_ave_def))  \n",
    "        precision_values_def.append(average_of_n_values(precision_ave_def)) \n",
    "        recall_values_def.append(average_of_n_values(recall_ave_def)) \n",
    "        f1_values_def.append(average_of_n_values(f1_ave_def)) \n",
    "\n",
    "        # end_time2 = time()\n",
    "        # totalTime2 = convert_seconds_to_minutes(end_time2 - startTime2)\n",
    "        # print(\"Total time 2 is: \", totalTime2)                \n",
    "            \n",
    "\n",
    "    '''\n",
    "    Combine values for deformation\n",
    "    '''\n",
    "    mae_values_array_def.append(mae_values_def)\n",
    "    mse_values_array_def.append(mse_values_def)\n",
    "    rmse_values_array_def.append(rmse_values_def)\n",
    "    r2_values_array_def.append(r2_values_def)\n",
    "    \n",
    "    accuracy_values_array_def.append(accuracy_values_def)\n",
    "    precision_values_array_def.append(precision_values_def)\n",
    "    recall_values_array_def.append(recall_values_def)\n",
    "    f1_values_array_def.append(f1_values_def)\n",
    "\n",
    "    # end_time3 = time()\n",
    "    # totalTime3 = convert_seconds_to_minutes(end_time3 - startTime3)\n",
    "    # print(\"Total time 3 is: \", totalTime3)                    \n",
    "    \n",
    "    \n",
    "'''\n",
    "Print K-fold for deformation\n",
    "'''\n",
    "print(\"These are the def mae for each k-fold split\", k_fold_mae_def, \"\\n\")\n",
    "print(\"These are the def mse for each k-fold split\", k_fold_mse_def, \"\\n\")\n",
    "print(\"These are the def rmse for each k-fold split\", k_fold_rmse_def, \"\\n\")\n",
    "print(\"These are the def r2 for each k-fold split\", k_fold_r2_def, \"\\n\")\n",
    "\n",
    "print(\"These are the def accuracy for each k-fold split\", k_fold_accuracy_def, \"\\n\")\n",
    "print(\"These are the def precision for each k-fold split\", k_fold_precision_def, \"\\n\")\n",
    "print(\"These are the def recall for each k-fold split\", k_fold_recall_def, \"\\n\")\n",
    "print(\"These are the def f1 for each k-fold split\", k_fold_f1_def, \"\\n\")\n",
    "\n",
    "end_time4 = time()\n",
    "totalTime4 = convert_seconds_to_minutes(end_time4 - startTime4)\n",
    "print(\"Total completion time is: \", totalTime4)\n",
    "\n",
    "'''\n",
    "Plot for deformation\n",
    "'''\n",
    "plot_accuracy_bar_graph(mae_values_array_def, dist_measure_strings, \"mae_deformation_plot\", \"mae_deformation_plot\")\n",
    "plot_accuracy_bar_graph(mse_values_array_def, dist_measure_strings, \"mse_deformation_plot\", \"mse_deformation_plot\")\n",
    "plot_accuracy_bar_graph(rmse_values_array_def, dist_measure_strings, \"rmse_deformation_plot\", \"rmse_deformation_plot\")\n",
    "plot_accuracy_bar_graph(r2_values_array_def, dist_measure_strings, \"r2_deformation_plot\", \"r2_deformation_plot\")\n",
    "\n",
    "plot_accuracy_bar_graph(accuracy_values_array_def, dist_measure_strings, \"accuracy_deformation_plot\", \"accuracy_deformation_plot\")\n",
    "plot_accuracy_bar_graph(precision_values_array_def, dist_measure_strings, \"precision_deformation_plot\", \"precision_deformation_plot\")\n",
    "plot_accuracy_bar_graph(recall_values_array_def, dist_measure_strings, \"recall_deformation_plot\", \"recall_deformation_plot\")\n",
    "plot_accuracy_bar_graph(f1_values_array_def, dist_measure_strings, \"f1_deformation_plot\", \"f1_deformation_plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Full Texture code for TSC_LS\n",
    "'''\n",
    "for dist_measure in dist_measure_strings:\n",
    "    startTime4 = time() \n",
    "    \n",
    "    '''\n",
    "    Metric value for texture\n",
    "    '''\n",
    "    mae_values_tex = []\n",
    "    mse_values_tex = []\n",
    "    rmse_values_tex = []\n",
    "    r2_values_tex = []\n",
    "    \n",
    "    accuracy_values_tex = []\n",
    "    precision_values_tex = []\n",
    "    recall_values_tex = []\n",
    "    f1_values_tex = []\n",
    "    \n",
    "    for shapelets_len in shapelets_len_values:\n",
    "        \n",
    "        startTime3 = time() # Add a timer. \n",
    "        \n",
    "        '''\n",
    "        Metric ave for texture\n",
    "        '''\n",
    "        mae_ave_tex = []\n",
    "        mse_ave_tex = []\n",
    "        rmse_ave_tex = []\n",
    "        r2_ave_tex = []\n",
    "        \n",
    "        accuracy_ave_tex = []\n",
    "        precision_ave_tex = []\n",
    "        recall_ave_tex = []\n",
    "        f1_ave_tex = []\n",
    "        \n",
    "        for fold_no in range(1,n_splits+1):\n",
    "            \n",
    "            startTime2 = time() # Add a timer.\n",
    "\n",
    "            '''\n",
    "            Train the texture data\n",
    "            '''\n",
    "            # Used for training the model\n",
    "            newtrain_tex = pd.read_csv(os.path.join(directory_path_param, 'train_fold_tex_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newtrain_tex = newtrain_tex.columns[1:-1]\n",
    "            combined_array_newtrain_tex = []\n",
    "\n",
    "            for column in columns_to_combine_newtrain_tex:\n",
    "                new_array_tex = newtrain_tex[column].to_numpy()\n",
    "                combined_array_newtrain_tex.append(new_array_tex)\n",
    "\n",
    "            combined_array_newtrain_tex = np.column_stack(combined_array_newtrain_tex)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_train_tex = np.array([label_map[label] for label in newtrain_tex.iloc[:,-1].to_numpy()])\n",
    "            new_X_train_tex = combined_array_newtrain_tex.reshape(new_y_train_tex.size, columns_to_combine_newtrain_tex.size,1)\n",
    "            new_X_train_tex, scaler_tex = normalize_data(new_X_train_tex)\n",
    "            \n",
    "        \n",
    "            n_ts_tex, n_channels_tex, len_ts_tex = new_X_train_tex.shape\n",
    "            loss_func_tex = nn.CrossEntropyLoss()\n",
    "            num_classes_tex = len(set(new_y_train_tex))\n",
    "            # learn 2 shapelets of length 130\n",
    "            shapelets_size_and_len_tex = {1: shapelets_len}\n",
    "            # dist_measure_tex = \"euclidean\"\n",
    "            # dist_measure_tex = \"cross-correlation\"\n",
    "            dist_measure_tex = dist_measure\n",
    "            \n",
    "            \n",
    "            lr_tex = 1e-2\n",
    "            wd_tex = 1e-3\n",
    "            epsilon_tex = 1e-7\n",
    "            learning_shapelets_tex = LearningShapelets(shapelets_size_and_len=shapelets_size_and_len_tex,\n",
    "                                                in_channels=n_channels_tex,\n",
    "                                                num_classes=num_classes_tex,\n",
    "                                                loss_func=loss_func_tex,\n",
    "                                                to_cuda=False,\n",
    "                                                verbose=1,\n",
    "                                                dist_measure=dist_measure_tex)\n",
    "            \n",
    "            for i, (shapelets_size, num_shapelets) in enumerate(shapelets_size_and_len_tex.items()):\n",
    "                weights_block = get_weights_via_kmeans(new_X_train_tex, shapelets_size, num_shapelets)\n",
    "                learning_shapelets_tex.set_shapelet_weights_of_block(i, weights_block)\n",
    "        \n",
    "            optimizer = optim.Adam(learning_shapelets_tex.model.parameters(), lr=lr_tex, weight_decay=wd_tex, eps=epsilon_tex)\n",
    "            learning_shapelets_tex.set_optimizer(optimizer)\n",
    "\n",
    "            losses = learning_shapelets_tex.fit(new_X_train_tex, new_y_train_tex, epochs=2000, batch_size=256, shuffle=False, drop_last=False)\n",
    "            print(\"Training is Done\")\n",
    "                \n",
    "            '''\n",
    "            Test the texture data\n",
    "            '''                          \n",
    "            newval_tex = pd.read_csv(os.path.join(directory_path_param, 'val_fold_tex_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newval_tex = newval_tex.columns[1:-1]\n",
    "            combined_array_newval_tex = []\n",
    "            for column in columns_to_combine_newval_tex:\n",
    "                new_array_tex = newval_tex[column].to_numpy()\n",
    "                combined_array_newval_tex.append(new_array_tex)\n",
    "            combined_array_newval_tex = np.column_stack(combined_array_newval_tex)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_val_tex = np.array([label_map[label] for label in newval_tex.iloc[:,-1].to_numpy()])\n",
    "            new_X_val_tex = combined_array_newval_tex.reshape(new_y_val_tex.size,columns_to_combine_newval_tex.size,1)\n",
    "            new_X_val_tex, scaler = normalize_data(new_X_val_tex)\n",
    "            \n",
    "            y_predlr_val_tex = np.array(eval_accuracy(learning_shapelets_tex,new_X_val_tex,new_y_val_tex)).reshape(-1)\n",
    "        \n",
    "            '''\n",
    "            Metrics For Texture\n",
    "            '''\n",
    "            mae_tex, mse_tex, rmse_tex, r2_tex, accuracy_tex, precision_tex, recall_tex, f1_tex, conf_matrix_tex, class_report_tex = calculate_metrics(new_y_val_tex, y_predlr_val_tex)\n",
    "            print(\"mae:\", mae_tex*100)\n",
    "            print(\"mse:\", mse_tex*100)\n",
    "            print(\"rmse:\", rmse_tex*100)\n",
    "            print(\"r2:\", r2_tex*100)\n",
    "            \n",
    "            print(\"Accuracy:\", accuracy_tex*100)\n",
    "            print(\"Precision:\", precision_tex*100)\n",
    "            print(\"Recall:\", recall_tex*100)\n",
    "            print(\"F1 Score:\", f1_tex*100)\n",
    "            print(\"Confusion Matrix:\\n\", conf_matrix_tex)\n",
    "            print(\"Classification report:\\n\", class_report_tex)\n",
    "            \n",
    "            mae_ave_tex.append(mae_tex)\n",
    "            mse_ave_tex.append(mse_tex)\n",
    "            rmse_ave_tex.append(rmse_tex)\n",
    "            r2_ave_tex.append(r2_tex)\n",
    "            \n",
    "            accuracy_ave_tex.append(accuracy_tex)\n",
    "            precision_ave_tex.append(precision_tex)\n",
    "            recall_ave_tex.append(recall_tex)\n",
    "            f1_ave_tex.append(f1_tex)  \n",
    "\n",
    "            \n",
    "            # end_time1 = time()\n",
    "            # totalTime1 = convert_seconds_to_minutes(end_time1 - startTime1)\n",
    "            # print(\"Total time 1 is: \", totalTime1)          \n",
    "            \n",
    "        '''\n",
    "        Combine K-fold for texture\n",
    "        '''    \n",
    "        k_fold_mae_tex.append(mae_ave_tex)\n",
    "        k_fold_mse_tex.append(mse_ave_tex)\n",
    "        k_fold_rmse_tex.append(rmse_ave_tex)\n",
    "        k_fold_r2_tex.append(r2_ave_tex)\n",
    "        \n",
    "        k_fold_accuracy_tex.append(accuracy_ave_tex)\n",
    "        k_fold_precision_tex.append(precision_ave_tex)\n",
    "        k_fold_recall_tex.append(recall_ave_tex)\n",
    "        k_fold_f1_tex.append(f1_ave_tex) \n",
    "        \n",
    "        '''\n",
    "        Combine values for texture\n",
    "        '''\n",
    "        mae_values_tex.append(average_of_n_values(mae_ave_tex)) \n",
    "        mse_values_tex.append(average_of_n_values(mse_ave_tex)) \n",
    "        rmse_values_tex.append(average_of_n_values(rmse_ave_tex)) \n",
    "        r2_values_tex.append(average_of_n_values(r2_ave_tex)) \n",
    "        \n",
    "        accuracy_values_tex.append(average_of_n_values(accuracy_ave_tex))  \n",
    "        precision_values_tex.append(average_of_n_values(precision_ave_tex)) \n",
    "        recall_values_tex.append(average_of_n_values(recall_ave_tex)) \n",
    "        f1_values_tex.append(average_of_n_values(f1_ave_tex)) \n",
    "    \n",
    "        # end_time2 = time()\n",
    "        # totalTime2 = convert_seconds_to_minutes(end_time2 - startTime2)\n",
    "        # print(\"Total time 2 is: \", totalTime2)                \n",
    "            \n",
    "    '''\n",
    "    Combine values for texture\n",
    "    '''    \n",
    "    mae_values_array_tex.append(mae_values_tex)\n",
    "    mse_values_array_tex.append(mse_values_tex)\n",
    "    rmse_values_array_tex.append(rmse_values_tex)\n",
    "    r2_values_array_tex.append(r2_values_tex)\n",
    "    \n",
    "    accuracy_values_array_tex.append(accuracy_values_tex)\n",
    "    precision_values_array_tex.append(precision_values_tex)\n",
    "    recall_values_array_tex.append(recall_values_tex)\n",
    "    f1_values_array_tex.append(f1_values_tex)    \n",
    "\n",
    "    # end_time3 = time()\n",
    "    # totalTime3 = convert_seconds_to_minutes(end_time3 - startTime3)\n",
    "    # print(\"Total time 3 is: \", totalTime3)                    \n",
    "    \n",
    "'''\n",
    "Print K-fold for texture\n",
    "'''\n",
    "print(\"These are the tex mae for each k-fold split\", k_fold_mae_tex, \"\\n\")\n",
    "print(\"These are the tex mse for each k-fold split\", k_fold_mse_tex, \"\\n\")\n",
    "print(\"These are the tex rmse for each k-fold split\", k_fold_rmse_tex, \"\\n\")\n",
    "print(\"These are the tex r2 for each k-fold split\", k_fold_r2_tex, \"\\n\")\n",
    "\n",
    "print(\"These are the tex accuracy for each k-fold split\", k_fold_accuracy_tex, \"\\n\")\n",
    "print(\"These are the tex precision for each k-fold split\", k_fold_precision_tex, \"\\n\")\n",
    "print(\"These are the tex recall for each k-fold split\", k_fold_recall_tex, \"\\n\")\n",
    "print(\"These are the tex f1 for each k-fold split\", k_fold_f1_tex, \"\\n\")\n",
    "\n",
    "end_time4 = time()\n",
    "totalTime4 = convert_seconds_to_minutes(end_time4 - startTime4)\n",
    "print(\"Total completion time is: \", totalTime4)\n",
    "\n",
    "'''\n",
    "Plot for texture\n",
    "'''\n",
    "plot_accuracy_bar_graph(mae_values_array_tex, dist_measure_strings, \"mae_texture_plot\", \"mae_texture_plot\")\n",
    "plot_accuracy_bar_graph(mse_values_array_tex, dist_measure_strings, \"mse_texture_plot\", \"mse_texture_plot\")\n",
    "plot_accuracy_bar_graph(rmse_values_array_tex, dist_measure_strings, \"rmse_texture_plot\", \"rmse_texture_plot\")\n",
    "plot_accuracy_bar_graph(r2_values_array_tex, dist_measure_strings, \"r2_texture_plot\", \"r2_texture_plot\")\n",
    "\n",
    "plot_accuracy_bar_graph(accuracy_values_array_tex, dist_measure_strings, \"accuracy_texture_plot\", \"accuracy_texture_plot\")\n",
    "plot_accuracy_bar_graph(precision_values_array_tex, dist_measure_strings, \"precision_texture_plot\", \"precision_texture_plot\")\n",
    "plot_accuracy_bar_graph(recall_values_array_tex, dist_measure_strings, \"recall_texture_plot\", \"recall_texture_plot\")\n",
    "plot_accuracy_bar_graph(f1_values_array_tex, dist_measure_strings, \"f1_texture_plot\", \"f1_texture_plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Both Deformation and Texture code for TSC_LS\n",
    "'''\n",
    "for dist_measure in dist_measure_strings:\n",
    "    startTime4 = time() \n",
    "    \n",
    "    '''\n",
    "    Metric value for deformation\n",
    "    '''\n",
    "    mae_values_def = []\n",
    "    mse_values_def = []\n",
    "    rmse_values_def = []\n",
    "    r2_values_def = []\n",
    "    \n",
    "    accuracy_values_def = []\n",
    "    precision_values_def = []\n",
    "    recall_values_def = []\n",
    "    f1_values_def = []\n",
    "    \n",
    "    '''\n",
    "    Metric value for texture\n",
    "    '''\n",
    "    mae_values_tex = []\n",
    "    mse_values_tex = []\n",
    "    rmse_values_tex = []\n",
    "    r2_values_tex = []\n",
    "    \n",
    "    accuracy_values_tex = []\n",
    "    precision_values_tex = []\n",
    "    recall_values_tex = []\n",
    "    f1_values_tex = []\n",
    "    \n",
    "    for shapelets_len in shapelets_len_values:\n",
    "        \n",
    "        startTime3 = time() # Add a timer. \n",
    "        \n",
    "        '''\n",
    "        Metric ave for deformation\n",
    "        '''\n",
    "        mae_ave_def = []\n",
    "        mse_ave_def = []\n",
    "        rmse_ave_def = []\n",
    "        r2_ave_def = []\n",
    "        \n",
    "        accuracy_ave_def = []\n",
    "        precision_ave_def = []\n",
    "        recall_ave_def = []\n",
    "        f1_ave_def = []\n",
    "    \n",
    "        '''\n",
    "        Metric ave for texture\n",
    "        '''\n",
    "        mae_ave_tex = []\n",
    "        mse_ave_tex = []\n",
    "        rmse_ave_tex = []\n",
    "        r2_ave_tex = []\n",
    "        \n",
    "        accuracy_ave_tex = []\n",
    "        precision_ave_tex = []\n",
    "        recall_ave_tex = []\n",
    "        f1_ave_tex = []\n",
    "        \n",
    "        for fold_no in range(1,n_splits+1):\n",
    "            \n",
    "            startTime2 = time() # Add a timer.\n",
    "\n",
    "            '''\n",
    "            Train the deformation data\n",
    "            '''\n",
    "            newtrain_def = pd.read_csv(os.path.join(directory_path_param, 'train_fold_def_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newtrain_def = newtrain_def.columns[1:-1]\n",
    "            combined_array_newtrain_def = []\n",
    "            \n",
    "            for column in columns_to_combine_newtrain_def:\n",
    "                new_array_def = newtrain_def[column].to_numpy()\n",
    "                combined_array_newtrain_def.append(new_array_def)\n",
    "\n",
    "            combined_array_newtrain_def = np.column_stack(combined_array_newtrain_def)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_train_def = np.array([label_map[label] for label in newtrain_def.iloc[:,-1].to_numpy()])\n",
    "            new_X_train_def = combined_array_newtrain_def.reshape(new_y_train_def.size, columns_to_combine_newtrain_def.size,1)\n",
    "            new_X_train_def, scaler_def = normalize_data(new_X_train_def)\n",
    "            \n",
    "        \n",
    "            n_ts_def, n_channels_def, len_ts_def = new_X_train_def.shape\n",
    "            loss_func_def = nn.CrossEntropyLoss()\n",
    "            num_classes_def = len(set(new_y_train_def))\n",
    "            # learn 2 shapelets of length 130\n",
    "            shapelets_size_and_len_def = {1: shapelets_len}\n",
    "            # dist_measure_def = \"euclidean\"\n",
    "            dist_measure_def = dist_measure\n",
    "            \n",
    "            lr_def = 1e-2\n",
    "            wd_def = 1e-3\n",
    "            epsilon_def = 1e-7\n",
    "        \n",
    "            learning_shapelets_def = LearningShapelets(shapelets_size_and_len=shapelets_size_and_len_def,\n",
    "                                                in_channels=n_channels_def,\n",
    "                                                num_classes=num_classes_def,\n",
    "                                                loss_func=loss_func_def,\n",
    "                                                to_cuda=False,\n",
    "                                                verbose=1,\n",
    "                                                dist_measure=dist_measure_def)\n",
    "\n",
    "            for i, (shapelets_size, num_shapelets) in enumerate(shapelets_size_and_len_def.items()):\n",
    "                weights_block = get_weights_via_kmeans(new_X_train_def, shapelets_size, num_shapelets)\n",
    "                learning_shapelets_def.set_shapelet_weights_of_block(i, weights_block)\n",
    "        \n",
    "            optimizer = optim.Adam(learning_shapelets_def.model.parameters(), lr=lr_def, weight_decay=wd_def, eps=epsilon_def)\n",
    "            learning_shapelets_def.set_optimizer(optimizer)\n",
    "            losses = learning_shapelets_def.fit(new_X_train_def, new_y_train_def, epochs=2000, batch_size=256, shuffle=False, drop_last=False)\n",
    "            print(\"Training is Done\")\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Train the texture data\n",
    "            '''\n",
    "            # Used for training the model\n",
    "            newtrain_tex = pd.read_csv(os.path.join(directory_path_param, 'train_fold_tex_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newtrain_tex = newtrain_tex.columns[1:-1]\n",
    "            combined_array_newtrain_tex = []\n",
    "\n",
    "            for column in columns_to_combine_newtrain_tex:\n",
    "                new_array_tex = newtrain_tex[column].to_numpy()\n",
    "                combined_array_newtrain_tex.append(new_array_tex)\n",
    "\n",
    "            combined_array_newtrain_tex = np.column_stack(combined_array_newtrain_tex)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_train_tex = np.array([label_map[label] for label in newtrain_tex.iloc[:,-1].to_numpy()])\n",
    "            new_X_train_tex = combined_array_newtrain_tex.reshape(new_y_train_tex.size, columns_to_combine_newtrain_tex.size,1)\n",
    "            new_X_train_tex, scaler_tex = normalize_data(new_X_train_tex)\n",
    "            \n",
    "        \n",
    "            n_ts_tex, n_channels_tex, len_ts_tex = new_X_train_tex.shape\n",
    "            loss_func_tex = nn.CrossEntropyLoss()\n",
    "            num_classes_tex = len(set(new_y_train_tex))\n",
    "            # learn 2 shapelets of length 130\n",
    "            shapelets_size_and_len_tex = {1: shapelets_len}\n",
    "            # dist_measure_tex = \"euclidean\"\n",
    "            # dist_measure_tex = \"cross-correlation\"\n",
    "            dist_measure_tex = dist_measure\n",
    "            \n",
    "            \n",
    "            lr_tex = 1e-2\n",
    "            wd_tex = 1e-3\n",
    "            epsilon_tex = 1e-7\n",
    "            learning_shapelets_tex = LearningShapelets(shapelets_size_and_len=shapelets_size_and_len_tex,\n",
    "                                                in_channels=n_channels_tex,\n",
    "                                                num_classes=num_classes_tex,\n",
    "                                                loss_func=loss_func_tex,\n",
    "                                                to_cuda=False,\n",
    "                                                verbose=1,\n",
    "                                                dist_measure=dist_measure_tex)\n",
    "            \n",
    "            for i, (shapelets_size, num_shapelets) in enumerate(shapelets_size_and_len_tex.items()):\n",
    "                weights_block = get_weights_via_kmeans(new_X_train_tex, shapelets_size, num_shapelets)\n",
    "                learning_shapelets_tex.set_shapelet_weights_of_block(i, weights_block)\n",
    "        \n",
    "            optimizer = optim.Adam(learning_shapelets_tex.model.parameters(), lr=lr_tex, weight_decay=wd_tex, eps=epsilon_tex)\n",
    "            learning_shapelets_tex.set_optimizer(optimizer)\n",
    "\n",
    "            losses = learning_shapelets_tex.fit(new_X_train_tex, new_y_train_tex, epochs=2000, batch_size=256, shuffle=False, drop_last=False)\n",
    "            print(\"Training is Done\")\n",
    "                \n",
    "            '''\n",
    "            Test the deformation data\n",
    "            '''\n",
    "            newval_def = pd.read_csv(os.path.join(directory_path_param, 'val_fold_def_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newval_def = newval_def.columns[1:-1]\n",
    "            combined_array_newval_def = []\n",
    "            for column in columns_to_combine_newval_def:\n",
    "                new_array_def = newval_def[column].to_numpy()\n",
    "                combined_array_newval_def.append(new_array_def)\n",
    "            combined_array_newval_def = np.column_stack(combined_array_newval_def)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_val_def = np.array([label_map[label] for label in newval_def.iloc[:,-1].to_numpy()])\n",
    "            new_X_val_def = combined_array_newval_def.reshape(new_y_val_def.size,columns_to_combine_newval_def.size,1)\n",
    "            new_X_val_def, scaler = normalize_data(new_X_val_def)\n",
    "\n",
    "            y_predlr_val_def = np.array(eval_accuracy(learning_shapelets_def,new_X_val_def,new_y_val_def)).reshape(-1)\n",
    "\n",
    "            '''\n",
    "            Test the texture data\n",
    "            '''                          \n",
    "            newval_tex = pd.read_csv(os.path.join(directory_path_param, 'val_fold_tex_' + str(fold_no) + '.csv'))\n",
    "            columns_to_combine_newval_tex = newval_tex.columns[1:-1]\n",
    "            combined_array_newval_tex = []\n",
    "            for column in columns_to_combine_newval_tex:\n",
    "                new_array_tex = newval_tex[column].to_numpy()\n",
    "                combined_array_newval_tex.append(new_array_tex)\n",
    "            combined_array_newval_tex = np.column_stack(combined_array_newval_tex)\n",
    "            label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "            new_y_val_tex = np.array([label_map[label] for label in newval_tex.iloc[:,-1].to_numpy()])\n",
    "            new_X_val_tex = combined_array_newval_tex.reshape(new_y_val_tex.size,columns_to_combine_newval_tex.size,1)\n",
    "            new_X_val_tex, scaler = normalize_data(new_X_val_tex)\n",
    "            \n",
    "            y_predlr_val_tex = np.array(eval_accuracy(learning_shapelets_tex,new_X_val_tex,new_y_val_tex)).reshape(-1)\n",
    "        \n",
    "            '''\n",
    "            Metrics For Deformation\n",
    "            '''   \n",
    "            mae_def, mse_def, rmse_def, r2_def, accuracy_def, precision_def, recall_def, f1_def, conf_matrix_def, class_report_def = calculate_metrics(new_y_val_def, y_predlr_val_def)\n",
    "            print(\"mae:\", mae_def*100)\n",
    "            print(\"mse:\", mse_def*100)\n",
    "            print(\"rmse:\", rmse_def*100)\n",
    "            print(\"r2:\", r2_def*100)\n",
    "            \n",
    "            print(\"Accuracy:\", accuracy_def*100)\n",
    "            print(\"Precision:\", precision_def*100)\n",
    "            print(\"Recall:\", recall_def*100)\n",
    "            print(\"F1 Score:\", f1_def*100)\n",
    "            print(\"Confusion Matrix:\\n\", conf_matrix_def)\n",
    "            print(\"Classification report:\\n\", class_report_def)\n",
    "            \n",
    "            mae_ave_def.append(mae_def)\n",
    "            mse_ave_def.append(mse_def)\n",
    "            rmse_ave_def.append(rmse_def)\n",
    "            r2_ave_def.append(r2_def)\n",
    "            \n",
    "            accuracy_ave_def.append(accuracy_def)\n",
    "            precision_ave_def.append(precision_def)\n",
    "            recall_ave_def.append(recall_def)\n",
    "            f1_ave_def.append(f1_def)    \n",
    "            \n",
    "            '''\n",
    "            Metrics For Texture\n",
    "            '''\n",
    "            mae_tex, mse_tex, rmse_tex, r2_tex, accuracy_tex, precision_tex, recall_tex, f1_tex, conf_matrix_tex, class_report_tex = calculate_metrics(new_y_val_tex, y_predlr_val_tex)\n",
    "            print(\"mae:\", mae_tex*100)\n",
    "            print(\"mse:\", mse_tex*100)\n",
    "            print(\"rmse:\", rmse_tex*100)\n",
    "            print(\"r2:\", r2_tex*100)\n",
    "            \n",
    "            print(\"Accuracy:\", accuracy_tex*100)\n",
    "            print(\"Precision:\", precision_tex*100)\n",
    "            print(\"Recall:\", recall_tex*100)\n",
    "            print(\"F1 Score:\", f1_tex*100)\n",
    "            print(\"Confusion Matrix:\\n\", conf_matrix_tex)\n",
    "            print(\"Classification report:\\n\", class_report_tex)\n",
    "            \n",
    "            mae_ave_tex.append(mae_tex)\n",
    "            mse_ave_tex.append(mse_tex)\n",
    "            rmse_ave_tex.append(rmse_tex)\n",
    "            r2_ave_tex.append(r2_tex)\n",
    "            \n",
    "            accuracy_ave_tex.append(accuracy_tex)\n",
    "            precision_ave_tex.append(precision_tex)\n",
    "            recall_ave_tex.append(recall_tex)\n",
    "            f1_ave_tex.append(f1_tex)  \n",
    "\n",
    "            \n",
    "            # end_time1 = time()\n",
    "            # totalTime1 = convert_seconds_to_minutes(end_time1 - startTime1)\n",
    "            # print(\"Total time 1 is: \", totalTime1)          \n",
    "            \n",
    "\n",
    "        '''\n",
    "        Combine K-fold for deformation\n",
    "        '''\n",
    "        k_fold_mae_def.append(mae_ave_def)\n",
    "        k_fold_mse_def.append(mse_ave_def)\n",
    "        k_fold_rmse_def.append(rmse_ave_def)\n",
    "        k_fold_r2_def.append(r2_ave_def)\n",
    "        \n",
    "        k_fold_accuracy_def.append(accuracy_ave_def)\n",
    "        k_fold_precision_def.append(precision_ave_def)\n",
    "        k_fold_recall_def.append(recall_ave_def)\n",
    "        k_fold_f1_def.append(f1_ave_def)\n",
    "        \n",
    "        '''\n",
    "        Combine K-fold for texture\n",
    "        '''    \n",
    "        k_fold_mae_tex.append(mae_ave_tex)\n",
    "        k_fold_mse_tex.append(mse_ave_tex)\n",
    "        k_fold_rmse_tex.append(rmse_ave_tex)\n",
    "        k_fold_r2_tex.append(r2_ave_tex)\n",
    "        \n",
    "        k_fold_accuracy_tex.append(accuracy_ave_tex)\n",
    "        k_fold_precision_tex.append(precision_ave_tex)\n",
    "        k_fold_recall_tex.append(recall_ave_tex)\n",
    "        k_fold_f1_tex.append(f1_ave_tex) \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Combine values for deformation\n",
    "        '''\n",
    "        mae_values_def.append(average_of_n_values(mae_ave_def)) \n",
    "        mse_values_def.append(average_of_n_values(mse_ave_def)) \n",
    "        rmse_values_def.append(average_of_n_values(rmse_ave_def)) \n",
    "        r2_values_def.append(average_of_n_values(r2_ave_def)) \n",
    "        \n",
    "        accuracy_values_def.append(average_of_n_values(accuracy_ave_def))  \n",
    "        precision_values_def.append(average_of_n_values(precision_ave_def)) \n",
    "        recall_values_def.append(average_of_n_values(recall_ave_def)) \n",
    "        f1_values_def.append(average_of_n_values(f1_ave_def)) \n",
    "\n",
    "        '''\n",
    "        Combine values for texture\n",
    "        '''\n",
    "        mae_values_tex.append(average_of_n_values(mae_ave_tex)) \n",
    "        mse_values_tex.append(average_of_n_values(mse_ave_tex)) \n",
    "        rmse_values_tex.append(average_of_n_values(rmse_ave_tex)) \n",
    "        r2_values_tex.append(average_of_n_values(r2_ave_tex)) \n",
    "        \n",
    "        accuracy_values_tex.append(average_of_n_values(accuracy_ave_tex))  \n",
    "        precision_values_tex.append(average_of_n_values(precision_ave_tex)) \n",
    "        recall_values_tex.append(average_of_n_values(recall_ave_tex)) \n",
    "        f1_values_tex.append(average_of_n_values(f1_ave_tex)) \n",
    "    \n",
    "        # end_time2 = time()\n",
    "        # totalTime2 = convert_seconds_to_minutes(end_time2 - startTime2)\n",
    "        # print(\"Total time 2 is: \", totalTime2)                \n",
    "            \n",
    "\n",
    "    '''\n",
    "    Combine values for deformation\n",
    "    '''\n",
    "    mae_values_array_def.append(mae_values_def)\n",
    "    mse_values_array_def.append(mse_values_def)\n",
    "    rmse_values_array_def.append(rmse_values_def)\n",
    "    r2_values_array_def.append(r2_values_def)\n",
    "    \n",
    "    accuracy_values_array_def.append(accuracy_values_def)\n",
    "    precision_values_array_def.append(precision_values_def)\n",
    "    recall_values_array_def.append(recall_values_def)\n",
    "    f1_values_array_def.append(f1_values_def)\n",
    "    \n",
    "    '''\n",
    "    Combine values for texture\n",
    "    '''    \n",
    "    mae_values_array_tex.append(mae_values_tex)\n",
    "    mse_values_array_tex.append(mse_values_tex)\n",
    "    rmse_values_array_tex.append(rmse_values_tex)\n",
    "    r2_values_array_tex.append(r2_values_tex)\n",
    "    \n",
    "    accuracy_values_array_tex.append(accuracy_values_tex)\n",
    "    precision_values_array_tex.append(precision_values_tex)\n",
    "    recall_values_array_tex.append(recall_values_tex)\n",
    "    f1_values_array_tex.append(f1_values_tex)    \n",
    "\n",
    "    # end_time3 = time()\n",
    "    # totalTime3 = convert_seconds_to_minutes(end_time3 - startTime3)\n",
    "    # print(\"Total time 3 is: \", totalTime3)                    \n",
    "    \n",
    "    \n",
    "'''\n",
    "Print K-fold for deformation\n",
    "'''\n",
    "print(\"These are the def mae for each k-fold split\", k_fold_mae_def, \"\\n\")\n",
    "print(\"These are the def mse for each k-fold split\", k_fold_mse_def, \"\\n\")\n",
    "print(\"These are the def rmse for each k-fold split\", k_fold_rmse_def, \"\\n\")\n",
    "print(\"These are the def r2 for each k-fold split\", k_fold_r2_def, \"\\n\")\n",
    "\n",
    "print(\"These are the def accuracy for each k-fold split\", k_fold_accuracy_def, \"\\n\")\n",
    "print(\"These are the def precision for each k-fold split\", k_fold_precision_def, \"\\n\")\n",
    "print(\"These are the def recall for each k-fold split\", k_fold_recall_def, \"\\n\")\n",
    "print(\"These are the def f1 for each k-fold split\", k_fold_f1_def, \"\\n\")\n",
    "\n",
    "'''\n",
    "Print K-fold for texture\n",
    "'''\n",
    "print(\"These are the tex mae for each k-fold split\", k_fold_mae_tex, \"\\n\")\n",
    "print(\"These are the tex mse for each k-fold split\", k_fold_mse_tex, \"\\n\")\n",
    "print(\"These are the tex rmse for each k-fold split\", k_fold_rmse_tex, \"\\n\")\n",
    "print(\"These are the tex r2 for each k-fold split\", k_fold_r2_tex, \"\\n\")\n",
    "\n",
    "print(\"These are the tex accuracy for each k-fold split\", k_fold_accuracy_tex, \"\\n\")\n",
    "print(\"These are the tex precision for each k-fold split\", k_fold_precision_tex, \"\\n\")\n",
    "print(\"These are the tex recall for each k-fold split\", k_fold_recall_tex, \"\\n\")\n",
    "print(\"These are the tex f1 for each k-fold split\", k_fold_f1_tex, \"\\n\")\n",
    "\n",
    "end_time4 = time()\n",
    "totalTime4 = convert_seconds_to_minutes(end_time4 - startTime4)\n",
    "print(\"Total completion time is: \", totalTime4)\n",
    "\n",
    "\n",
    "'''\n",
    "Plot for deformation\n",
    "'''\n",
    "plot_accuracy_bar_graph(mae_values_array_def, dist_measure_strings, \"mae_deformation_plot\", \"mae_deformation_plot\")\n",
    "plot_accuracy_bar_graph(mse_values_array_def, dist_measure_strings, \"mse_deformation_plot\", \"mse_deformation_plot\")\n",
    "plot_accuracy_bar_graph(rmse_values_array_def, dist_measure_strings, \"rmse_deformation_plot\", \"rmse_deformation_plot\")\n",
    "plot_accuracy_bar_graph(r2_values_array_def, dist_measure_strings, \"r2_deformation_plot\", \"r2_deformation_plot\")\n",
    "\n",
    "plot_accuracy_bar_graph(accuracy_values_array_def, dist_measure_strings, \"accuracy_deformation_plot\", \"accuracy_deformation_plot\")\n",
    "plot_accuracy_bar_graph(precision_values_array_def, dist_measure_strings, \"precision_deformation_plot\", \"precision_deformation_plot\")\n",
    "plot_accuracy_bar_graph(recall_values_array_def, dist_measure_strings, \"recall_deformation_plot\", \"recall_deformation_plot\")\n",
    "plot_accuracy_bar_graph(f1_values_array_def, dist_measure_strings, \"f1_deformation_plot\", \"f1_deformation_plot\")\n",
    "\n",
    "'''\n",
    "Plot for texture\n",
    "'''\n",
    "plot_accuracy_bar_graph(mae_values_array_tex, dist_measure_strings, \"mae_texture_plot\", \"mae_texture_plot\")\n",
    "plot_accuracy_bar_graph(mse_values_array_tex, dist_measure_strings, \"mse_texture_plot\", \"mse_texture_plot\")\n",
    "plot_accuracy_bar_graph(rmse_values_array_tex, dist_measure_strings, \"rmse_texture_plot\", \"rmse_texture_plot\")\n",
    "plot_accuracy_bar_graph(r2_values_array_tex, dist_measure_strings, \"r2_texture_plot\", \"r2_texture_plot\")\n",
    "\n",
    "plot_accuracy_bar_graph(accuracy_values_array_tex, dist_measure_strings, \"accuracy_texture_plot\", \"accuracy_texture_plot\")\n",
    "plot_accuracy_bar_graph(precision_values_array_tex, dist_measure_strings, \"precision_texture_plot\", \"precision_texture_plot\")\n",
    "plot_accuracy_bar_graph(recall_values_array_tex, dist_measure_strings, \"recall_texture_plot\", \"recall_texture_plot\")\n",
    "plot_accuracy_bar_graph(f1_values_array_tex, dist_measure_strings, \"f1_texture_plot\", \"f1_texture_plot\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
